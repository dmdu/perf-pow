Finite Element FAS Performance Sampler on process grid [1 1 3] = 3
Max memory per MPI rank: 0.007410 GB
Small Test G[   12   12   16]
Q2 G[   12   12   16] P[  1  1  3]  5.620e-02 s    2.839861 GF    0.366980 MEq/s
Large Test G[   24   32   32]
Q2  0 e_max 1.22e-01(0.0) e_L2 8.73e-02(0.0) r_2 1.20e-08(0.0) G[    3    4    4] L[   3   4   4] P[  1  1  1]
Q2  1 e_max 2.21e-02(2.5) e_L2 1.21e-02(2.9) r_2 4.73e-03(-18.6) G[    6    8    8] L[   6   8   4] P[  1  1  2]
Q2  2 e_max 2.72e-03(3.0) e_L2 1.54e-03(3.0) r_2 2.09e-03(1.2) G[   12   16   16] L[  12  16   6] P[  1  1  3]
Q2  3 e_max 3.56e-04(2.9) e_L2 1.93e-04(3.0) r_2 9.08e-04(1.2) G[   24   32   32] L[  24  32  11] P[  1  1  3]
Q2 G[   24   32   32] P[  1  1  3]  5.914e-01 s    3.473459 GF    0.350051 MEq/s
Max memory per MPI rank: 0.023396 GB
Starting performance sampling
Q2 G[   12   12   16] P[  1  1  3]  5.610e-02 s    2.844869 GF    0.367627 MEq/s
Q2 G[   12   12   16] P[  1  1  3]  5.509e-02 s    2.896635 GF    0.374414 MEq/s
Q2 G[   12   12   16] P[  1  1  3]  5.504e-02 s    2.899006 GF    0.374720 MEq/s
Q2 G[   12   12   16] P[  1  1  3]  5.497e-02 s    2.902753 GF    0.375204 MEq/s
Q2 G[   12   12   16] P[  1  1  3]  5.511e-02 s    2.895545 GF    0.374273 MEq/s
Q2 G[   16   16   16] P[  1  1  3]  7.456e-02 s    3.484344 GF    0.482000 MEq/s
Q2 G[   16   16   16] P[  1  1  3]  7.398e-02 s    3.510728 GF    0.485785 MEq/s
Q2 G[   16   16   16] P[  1  1  3]  7.384e-02 s    3.517439 GF    0.486713 MEq/s
Q2 G[   16   16   16] P[  1  1  3]  7.383e-02 s    3.517530 GF    0.486726 MEq/s
Q2 G[   16   16   16] P[  1  1  3]  7.380e-02 s    3.519109 GF    0.486944 MEq/s
Q2 G[   16   16   24] P[  1  1  3]  1.017e-01 s    3.858204 GF    0.524644 MEq/s
Q2 G[   16   16   24] P[  1  1  3]  1.003e-01 s    3.909186 GF    0.531722 MEq/s
Q2 G[   16   16   24] P[  1  1  3]  1.001e-01 s    3.918570 GF    0.532998 MEq/s
Q2 G[   16   16   24] P[  1  1  3]  1.002e-01 s    3.914412 GF    0.532432 MEq/s
Q2 G[   16   16   24] P[  1  1  3]  1.002e-01 s    3.913016 GF    0.532243 MEq/s
Q2 G[   16   16   32] P[  1  1  3]  1.384e-01 s    3.751542 GF    0.511534 MEq/s
Q2 G[   16   16   32] P[  1  1  3]  1.369e-01 s    3.789709 GF    0.516879 MEq/s
Q2 G[   16   16   32] P[  1  1  3]  1.376e-01 s    3.772240 GF    0.514497 MEq/s
Q2 G[   16   16   32] P[  1  1  3]  1.367e-01 s    3.795690 GF    0.517695 MEq/s
Q2 G[   16   16   32] P[  1  1  3]  1.369e-01 s    3.789564 GF    0.516859 MEq/s
Q2 G[   16   24   32] P[  1  1  3]  2.143e-01 s    3.679249 GF    0.490467 MEq/s
Q2 G[   16   24   32] P[  1  1  3]  2.136e-01 s    3.690772 GF    0.492134 MEq/s
Q2 G[   16   24   32] P[  1  1  3]  2.128e-01 s    3.703306 GF    0.493805 MEq/s
Q2 G[   16   24   32] P[  1  1  3]  2.131e-01 s    3.699047 GF    0.493237 MEq/s
Q2 G[   16   24   32] P[  1  1  3]  2.128e-01 s    3.703219 GF    0.493794 MEq/s
Q2 G[   24   32   32] P[  1  1  3]  4.320e-01 s    3.657384 GF    0.479159 MEq/s
Q2 G[   24   32   32] P[  1  1  3]  4.305e-01 s    3.669771 GF    0.480908 MEq/s
Q2 G[   24   32   32] P[  1  1  3]  4.300e-01 s    3.673150 GF    0.481351 MEq/s
Q2 G[   24   32   32] P[  1  1  3]  4.304e-01 s    3.670537 GF    0.481008 MEq/s
Q2 G[   24   32   32] P[  1  1  3]  4.299e-01 s    3.674038 GF    0.481467 MEq/s
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/opt/hpgmg/arm64/bin/hpgmg-fe on a arm64 named head.hpc-dev.utahstud-pg0.utah.cloudlab.us with 3 processors, by root Tue Oct 20 08:54:15 2015
Using Petsc Development GIT revision: v3.6.1-1137-gb19ca9c  GIT Date: 2015-09-28 11:20:58 -0500

                         Max       Max/Min        Avg      Total 
Time (sec):           8.166e+00      1.00000   8.166e+00
Objects:              1.021e+03      2.50860   6.890e+02
Flops:                1.165e+10      1.16136   1.060e+10  3.180e+10
Flops/sec:            1.426e+09      1.16136   1.298e+09  3.894e+09
MPI Messages:         2.439e+03      1.55747   1.962e+03  5.885e+03
MPI Message Lengths:  1.307e+07      1.07930   6.482e+03  3.815e+07
MPI Reductions:       5.040e+02      1.47801

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 8.1662e+00 100.0%  3.1803e+10 100.0%  5.885e+03 100.0%  6.482e+03      100.0%  4.487e+02  89.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

                      93 0.0 5.3581e+00 0.0 6.44e+09 0.0 2.8e+03 6.3e+03 2.7e+01 43 40 47 46  5  43 40 47 46  6  2392
OpRestrictState      214 2.5 4.0839e-02 5.4 0.00e+00 0.0 6.6e+02 6.6e+03 0.0e+00  0  0 11 11  0   0  0 11 11  0     0
OpRestrictResid      314 2.2 1.3783e-01 1.2 1.37e+07 1.1 1.1e+03 8.0e+03 0.0e+00  2  0 19 23  0   2  0 19 23  0   282
OpInterpolate        314 2.2 8.4579e-01 6.3 1.37e+07 1.1 9.7e+02 8.5e+03 0.0e+00  6  0 16 22  0   6  0 16 22  0    46
OpForcing              8 1.0 2.3629e-01 1.0 2.20e+08 1.1 4.0e+01 7.5e+03 0.0e+00  3  2  1  1  0   3  2  1  1  0  2687
OpIntegNorms           4 2.0 9.2457e-02 1.0 7.36e+07 1.1 5.0e+00 1.3e+04 5.3e+00  1  1  0  0  1   1  1  0  0  1  2271
OpGetDiagonal         32 2.3 1.9321e+00 1.0 3.70e+09 1.1 1.2e+02 4.1e+03 0.0e+00 23 33  2  1  0  23 33  2  1  0  5413
SFSetGraph           160 1.9 9.3646e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFBcastBegin        3200 5.4 2.5418e-02 1.4 0.00e+00 0.0 2.4e+03 6.1e+03 0.0e+00  0  0 41 39  0   0  0 41 39  0     0
SFBcastEnd          3200 5.4 7.3843e-0123.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0
SFReduceBegin       3450 4.9 1.0019e-01 1.9 0.00e+00 0.0 3.5e+03 6.7e+03 0.0e+00  1  0 59 61  0   1  0 59 61  0     0
SFReduceEnd         3450 4.9 6.2086e-0121.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
BuildTwoSided        128 2.1 7.3408e-0214.8 0.00e+00 0.0 1.4e+02 4.0e+00 0.0e+00  0  0  2  0  0   0  0  2  0  0     0
VecDot               132 0.0 2.4652e-04 0.0 6.62e+04 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   268
VecTDot             3008 0.0 3.7811e-03 0.0 2.24e+06 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   591
VecNorm              112 3.1 4.6918e-03 1.6 9.82e+05 1.2 0.0e+00 0.0e+00 5.8e+01  0  0  0  0 12   0  0  0  0 13   587
VecCopy              560 5.1 2.2323e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              6898 5.2 1.5680e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY             330430.0 9.0847e-03 2.0 5.09e+06 2.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1130
VecAYPX             2940 5.0 4.2657e-02 1.2 1.24e+07 1.2 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   782
VecAXPBYCZ           428 2.5 1.7284e-02 1.2 1.25e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2061
VecPointwiseMult    2590 7.4 3.6170e-02 1.2 5.90e+06 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   434
MatMult             2426 7.2 4.0501e+00 1.2 5.92e+09 1.2 2.1e+03 4.7e+03 0.0e+00 46 50 35 25  0  46 50 35 25  0  3889
KSPSetUp              32 2.3 3.5417e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             560 3.3 4.1693e+00 1.2 5.95e+09 1.2 2.1e+03 4.7e+03 0.0e+00 47 50 35 25  0  47 50 35 25  0  3798
PCSetUp               32 2.3 2.2173e-05 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCApply             2590 7.4 1.9756e+00 1.0 3.70e+09 1.1 1.2e+02 4.1e+03 3.9e+01 24 33  2  1  8  24 33  2  1  9  5302
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

Star Forest Bipartite Graph   288            288       237568     0
              Vector   508            508     39522288     0
              Matrix    32             32        91904     0
    Distributed Mesh    64             64       288768     0
     Discrete System    64             64        54272     0
       Krylov Solver    32             32        39680     0
      Preconditioner    32             32        27136     0
              Viewer     1              0            0     0
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 0.000124168
Average time for zero size MPI_Send(): 4.4028e-05
#PETSc Option Table entries:
-local 1e3
-log_summary
-op_type poisson2
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=0 --prefix=/opt/petsc --download-f2cblaslapack=1 --with-shared-libraries=0
-----------------------------------------
Libraries compiled on Tue Sep 29 11:48:00 2015 on head.hpgmg-gcc520.utahstud-pg0.utah.cloudlab.us 
Machine characteristics: Linux-3.13.0-40-generic-aarch64-with-Ubuntu-14.04-trusty
Using PETSc directory: /opt/petsc-src
Using PETSc arch: arm64
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -g -O  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90  -Wall -Wno-unused-variable -ffree-line-length-0 -Wno-unused-dummy-argument -g -O   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/opt/petsc-src/arm64/include -I/opt/petsc-src/include -I/opt/petsc-src/include -I/opt/petsc-src/arm64/include -I/opt/openmpi/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/opt/petsc-src/arm64/lib -L/opt/petsc-src/arm64/lib -lpetsc -Wl,-rpath,/opt/petsc/lib -L/opt/petsc/lib -lf2clapack -lf2cblas -lm -lssl -lcrypto -lm -L/opt/openmpi/lib -L/opt/gcc520/lib/gcc/aarch64-unknown-linux-gnu/5.2.0 -L/opt/gcc520/lib/gcc -L/opt/gcc520/lib64 -L/lib/aarch64-linux-gnu -L/usr/lib/aarch64-linux-gnu -L/opt/gcc520/lib -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/opt/openmpi/lib -lgfortran -lm -lmpi_cxx -lstdc++ -L/opt/openmpi/lib -L/opt/gcc520/lib/gcc/aarch64-unknown-linux-gnu/5.2.0 -L/opt/gcc520/lib/gcc -L/opt/gcc520/lib64 -L/lib/aarch64-linux-gnu -L/usr/lib/aarch64-linux-gnu -L/opt/gcc520/lib -ldl -Wl,-rpath,/opt/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

