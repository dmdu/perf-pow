Finite Element FAS Performance Sampler on process grid [1 2 3] = 6
Max memory per MPI rank: 0.007885 GB
Small Test G[   16   16   16]
Q2 G[   16   16   16] P[  1  2  3]  4.441e-02 s    5.842850 GF    0.808138 MEq/s
Large Test G[   32   32   48]
Q2  0 e_max 3.17e-01(0.0) e_L2 3.10e-01(0.0) r_2 1.43e-14(0.0) G[    2    2    3] L[   2   2   3] P[  1  1  1]
Q2  1 e_max 8.73e-02(1.9) e_L2 4.86e-02(2.7) r_2 6.99e-03(0.0) G[    4    4    6] L[   4   4   3] P[  1  1  2]
Q2  2 e_max 1.22e-02(2.8) e_L2 6.41e-03(2.9) r_2 3.87e-03(0.9) G[    8    8   12] L[   8   4   4] P[  1  2  3]
Q2  3 e_max 1.54e-03(3.0) e_L2 8.09e-04(3.0) r_2 1.54e-03(1.3) G[   16   16   24] L[  16   8   8] P[  1  2  3]
Q2  4 e_max 1.91e-04(3.0) e_L2 1.01e-04(3.0) r_2 6.66e-04(1.2) G[   32   32   48] L[  32  16  16] P[  1  2  3]
Q2 G[   32   32   48] P[  1  2  3]  5.555e-01 s    7.313835 GF    0.737707 MEq/s
Max memory per MPI rank: 0.022938 GB
Starting performance sampling
Q2 G[   16   16   16] P[  1  2  3]  4.257e-02 s    6.098883 GF    0.843551 MEq/s
Q2 G[   16   16   16] P[  1  2  3]  4.168e-02 s    6.227293 GF    0.861550 MEq/s
Q2 G[   16   16   16] P[  1  2  3]  4.164e-02 s    6.236133 GF    0.862773 MEq/s
Q2 G[   16   16   16] P[  1  2  3]  4.170e-02 s    6.225977 GF    0.861368 MEq/s
Q2 G[   16   16   16] P[  1  2  3]  4.187e-02 s    6.199971 GF    0.857770 MEq/s
Q2 G[   16   16   24] P[  1  2  3]  5.833e-02 s    6.725545 GF    0.914434 MEq/s
Q2 G[   16   16   24] P[  1  2  3]  5.777e-02 s    6.789683 GF    0.923406 MEq/s
Q2 G[   16   16   24] P[  1  2  3]  5.776e-02 s    6.791112 GF    0.923600 MEq/s
Q2 G[   16   16   24] P[  1  2  3]  5.767e-02 s    6.802200 GF    0.925108 MEq/s
Q2 G[   16   16   24] P[  1  2  3]  5.763e-02 s    6.804788 GF    0.925460 MEq/s
Q2 G[   16   24   32] P[  1  2  3]  1.210e-01 s    6.515608 GF    0.868494 MEq/s
Q2 G[   16   24   32] P[  1  2  3]  1.202e-01 s    6.557212 GF    0.874273 MEq/s
Q2 G[   16   24   32] P[  1  2  3]  1.200e-01 s    6.568571 GF    0.875787 MEq/s
Q2 G[   16   24   32] P[  1  2  3]  1.201e-01 s    6.563277 GF    0.875081 MEq/s
Q2 G[   16   24   32] P[  1  2  3]  1.199e-01 s    6.571469 GF    0.876174 MEq/s
Q2 G[   24   24   32] P[  1  2  3]  1.819e-01 s    6.507499 GF    0.857782 MEq/s
Q2 G[   24   24   32] P[  1  2  3]  1.803e-01 s    6.564372 GF    0.865507 MEq/s
Q2 G[   24   24   32] P[  1  2  3]  1.801e-01 s    6.571341 GF    0.866426 MEq/s
Q2 G[   24   24   32] P[  1  2  3]  1.802e-01 s    6.567881 GF    0.865970 MEq/s
Q2 G[   24   24   32] P[  1  2  3]  1.803e-01 s    6.564711 GF    0.865552 MEq/s
Q2 G[   32   32   32] P[  1  2  3]  2.814e-01 s    7.370677 GF    0.975439 MEq/s
Q2 G[   32   32   32] P[  1  2  3]  2.794e-01 s    7.423995 GF    0.982755 MEq/s
Q2 G[   32   32   32] P[  1  2  3]  2.799e-01 s    7.410048 GF    0.980909 MEq/s
Q2 G[   32   32   32] P[  1  2  3]  2.796e-01 s    7.419399 GF    0.982147 MEq/s
Q2 G[   32   32   32] P[  1  2  3]  2.793e-01 s    7.423678 GF    0.982713 MEq/s
Q2 G[   32   32   48] P[  1  2  3]  4.025e-01 s    7.728329 GF    1.016827 MEq/s
Q2 G[   32   32   48] P[  1  2  3]  4.008e-01 s    7.758892 GF    1.021117 MEq/s
Q2 G[   32   32   48] P[  1  2  3]  4.002e-01 s    7.773160 GF    1.022995 MEq/s
Q2 G[   32   32   48] P[  1  2  3]  4.015e-01 s    7.756625 GF    1.020818 MEq/s
Q2 G[   32   32   48] P[  1  2  3]  4.002e-01 s    7.769651 GF    1.022533 MEq/s
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

/opt/hpgmg/arm64/bin/hpgmg-fe on a arm64 named head.hpc-dev.utahstud-pg0.utah.cloudlab.us with 6 processors, by root Tue Oct 20 08:54:48 2015
Using Petsc Development GIT revision: v3.6.1-1137-gb19ca9c  GIT Date: 2015-09-28 11:20:58 -0500

                         Max       Max/Min        Avg      Total 
Time (sec):           8.531e+00      1.00000   8.531e+00
Objects:              1.213e+03      2.19746   7.008e+02
Flops:                1.192e+10      1.12968   1.109e+10  6.652e+10
Flops/sec:            1.397e+09      1.12968   1.300e+09  7.798e+09
MPI Messages:         5.229e+03      2.28341   3.794e+03  2.277e+04
MPI Message Lengths:  1.662e+07      1.20925   3.948e+03  8.988e+07
MPI Reductions:       6.040e+02      1.36961

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 8.5305e+00 100.0%  6.6525e+10 100.0%  2.277e+04 100.0%  3.948e+03      100.0%  4.943e+02  81.8% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

                     111 0.0 5.6997e+00 0.0 7.18e+09 0.0 1.5e+04 3.7e+03 4.2e+01 55 53 64 60  7  55 53 64 60  8  6147
OpRestrictState      285 2.2 2.4913e-02 2.2 0.00e+00 0.0 1.5e+03 6.0e+03 0.0e+00  0  0  7 10  0   0  0  7 10  0     0
OpRestrictResid      403 2.0 1.4548e-01 1.2 1.41e+07 1.1 3.3e+03 5.5e+03 0.0e+00  2  0 15 20  0   2  0 15 20  0   552
OpInterpolate        403 2.0 7.1086e-01 4.9 1.41e+07 1.1 2.9e+03 5.9e+03 0.0e+00  7  0 13 19  0   7  0 13 19  0   113
OpForcing              8 1.0 2.4352e-01 1.0 2.24e+08 1.0 1.8e+02 4.4e+03 0.0e+00  3  2  1  1  0   3  2  1  1  0  5435
OpIntegNorms           5 1.7 8.8700e-02 1.0 7.03e+07 1.0 2.8e+01 5.7e+03 6.7e+00  1  1  0  0  1   1  1  0  0  1  4736
OpGetDiagonal         38 2.0 1.9658e+00 1.0 3.76e+09 1.1 6.1e+02 2.3e+03 0.0e+00 23 33  3  2  0  23 33  3  2  0 11095
SFSetGraph           196 1.7 1.0935e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFBcastBegin        3620 3.9 3.3846e-02 1.5 0.00e+00 0.0 1.0e+04 3.6e+03 0.0e+00  0  0 44 41  0   0  0 44 41  0     0
SFBcastEnd          3620 3.9 6.3741e-0110.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  6  0  0  0  0   6  0  0  0  0     0
SFReduceBegin       3946 3.7 8.4027e-02 1.7 0.00e+00 0.0 1.3e+04 4.2e+03 0.0e+00  1  0 56 59  0   1  0 56 59  0     0
SFReduceEnd         3946 3.7 3.9209e-01 6.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
BuildTwoSided        158 1.8 4.2062e-02 6.0 0.00e+00 0.0 4.4e+02 4.0e+00 0.0e+00  0  0  2  0  0   0  0  2  0  0     0
VecDot               150 0.0 2.3842e-04 0.0 5.08e+04 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   213
VecTDot             2784 0.0 2.8846e-03 0.0 1.48e+06 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   514
VecNorm              131 2.4 6.0987e-03 2.2 9.63e+05 1.1 0.0e+00 0.0e+00 6.5e+01  0  0  0  0 11   0  0  0  0 13   922
VecCopy              720 3.9 2.8536e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              7805 3.9 1.9871e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY             320417.5 1.1117e-02 1.5 4.41e+06 1.7 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1649
VecAYPX             3317 3.7 5.4152e-02 1.1 1.22e+07 1.1 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0  1260
VecAXPBYCZ           570 2.2 2.4933e-02 1.1 1.27e+07 1.1 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2988
VecPointwiseMult    2795 5.2 4.3062e-02 1.1 5.79e+06 1.2 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   741
MatMult             2607 5.1 4.2041e+00 1.1 6.05e+09 1.2 1.0e+04 2.8e+03 0.0e+00 46 49 45 31  0  46 49 45 31  0  7827
KSPSetUp              38 2.0 4.3635e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve             720 2.8 4.3471e+00 1.1 6.08e+09 1.2 1.0e+04 2.8e+03 0.0e+00 47 50 45 31  0  47 50 45 31  0  7608
PCSetUp               38 2.0 3.1233e-05 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCApply             2795 5.2 2.0147e+00 1.0 3.76e+09 1.1 6.1e+02 2.3e+03 4.3e+01 23 33  3  2  7  23 33  3  2  9 10842
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

Star Forest Bipartite Graph   348            348       287248     0
              Vector   598            598     39026512     0
              Matrix    38             38       109136     0
    Distributed Mesh    76             76       342912     0
     Discrete System    76             76        64448     0
       Krylov Solver    38             38        47168     0
      Preconditioner    38             38        32224     0
              Viewer     1              0            0     0
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 3.26157e-05
Average time for zero size MPI_Send(): 2.13385e-05
#PETSc Option Table entries:
-local 1e3
-log_summary
-op_type poisson2
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=0 --prefix=/opt/petsc --download-f2cblaslapack=1 --with-shared-libraries=0
-----------------------------------------
Libraries compiled on Tue Sep 29 11:48:00 2015 on head.hpgmg-gcc520.utahstud-pg0.utah.cloudlab.us 
Machine characteristics: Linux-3.13.0-40-generic-aarch64-with-Ubuntu-14.04-trusty
Using PETSc directory: /opt/petsc-src
Using PETSc arch: arm64
-----------------------------------------

Using C compiler: mpicc   -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -g -O  ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90  -Wall -Wno-unused-variable -ffree-line-length-0 -Wno-unused-dummy-argument -g -O   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/opt/petsc-src/arm64/include -I/opt/petsc-src/include -I/opt/petsc-src/include -I/opt/petsc-src/arm64/include -I/opt/openmpi/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/opt/petsc-src/arm64/lib -L/opt/petsc-src/arm64/lib -lpetsc -Wl,-rpath,/opt/petsc/lib -L/opt/petsc/lib -lf2clapack -lf2cblas -lm -lssl -lcrypto -lm -L/opt/openmpi/lib -L/opt/gcc520/lib/gcc/aarch64-unknown-linux-gnu/5.2.0 -L/opt/gcc520/lib/gcc -L/opt/gcc520/lib64 -L/lib/aarch64-linux-gnu -L/usr/lib/aarch64-linux-gnu -L/opt/gcc520/lib -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lgfortran -lm -Wl,-rpath,/opt/openmpi/lib -lgfortran -lm -lmpi_cxx -lstdc++ -L/opt/openmpi/lib -L/opt/gcc520/lib/gcc/aarch64-unknown-linux-gnu/5.2.0 -L/opt/gcc520/lib/gcc -L/opt/gcc520/lib64 -L/lib/aarch64-linux-gnu -L/usr/lib/aarch64-linux-gnu -L/opt/gcc520/lib -ldl -Wl,-rpath,/opt/openmpi/lib -lmpi -lgcc_s -lpthread -ldl 
-----------------------------------------

